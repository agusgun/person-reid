{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligned Re-ID Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aligned_reid.utils.utils import load_ckpt\n",
    "# import torch\n",
    "# from aligned_reid.utils.utils import load_state_dict\n",
    "# from aligned_reid.model.Model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'model_data/run1/model_weight.pth'\n",
    "# model = Model()\n",
    "\n",
    "\n",
    "# # map_location = (lambda storage, loc: storage)\n",
    "# # sd = torch.load(model_path, map_location=map_location)\n",
    "# # load_state_dict(model, sd)\n",
    "# # print('Loaded model weights from {}'.format(model_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person Re-ID Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ft_net_dense\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_model_path = 'model_data/model/ft_net_dense/net_last.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "use_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(network, model_path):\n",
    "    network.load_state_dict(torch.load(model_path))\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fliplr(img):\n",
    "    inv_idx = torch.arange(img.size(3)-1,-1,-1).long()  # N x C x H x W\n",
    "    img_flip = img.index_select(3,inv_idx)\n",
    "    return img_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 751 #market 151 class\n",
    "model_structure = ft_net_dense(n_classes)\n",
    "model_structure.load_state_dict(torch.load(densenet_model_path))\n",
    "model = model_structure.cuda()\n",
    "model.classifier.classifier = nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((256,128), interpolation=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../face_data'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path_dir = '../../face_data'\n",
    "image_path_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = datasets.ImageFolder(image_path_dir, data_transforms)\n",
    "dataloader = torch.utils.data.DataLoader(image_dataset, batch_size=16, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, dataloader):\n",
    "    features = torch.FloatTensor()\n",
    "    count = 0\n",
    "    for data in dataloader: # batch_size image\n",
    "        img, label = data\n",
    "        n, c, h, w = img.size() # number, channel, height, width\n",
    "        count += n\n",
    "        \n",
    "        ff = torch.FloatTensor(n, 512).zero_()\n",
    "        \n",
    "        for i in range(2):\n",
    "            if(i==1):\n",
    "                img = fliplr(img)\n",
    "            input_img = Variable(img.cuda())\n",
    "            \n",
    "            outputs = model(input_img) \n",
    "            f = outputs.data.cpu().float()\n",
    "            ff = ff+f\n",
    "        \n",
    "        fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n",
    "        ff = ff.div(fnorm.expand_as(ff))\n",
    "\n",
    "        features = torch.cat((features,ff), 0)\n",
    "        print(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hehe = 'ha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.58 µs\n",
      "tensor([[ 0.0101,  0.0612,  0.0000,  ...,  0.0527,  0.0000,  0.0000],\n",
      "        [ 0.0371,  0.0000,  0.0000,  ..., -0.0044,  0.0449, -0.0328],\n",
      "        [-0.0331, -0.0598,  0.0000,  ...,  0.0817, -0.0726, -0.0312],\n",
      "        ...,\n",
      "        [-0.0112,  0.0000,  0.0000,  ...,  0.0159, -0.0753,  0.0000],\n",
      "        [-0.0232,  0.0000, -0.1559,  ..., -0.0853,  0.0000,  0.0180],\n",
      "        [ 0.0073, -0.0572,  0.0000,  ..., -0.0326,  0.0000,  0.0044]])\n",
      "tensor([[ 0.0101,  0.0612,  0.0000,  ...,  0.0527,  0.0000,  0.0000],\n",
      "        [ 0.0371,  0.0000,  0.0000,  ..., -0.0044,  0.0449, -0.0328],\n",
      "        [-0.0331, -0.0598,  0.0000,  ...,  0.0817, -0.0726, -0.0312],\n",
      "        ...,\n",
      "        [ 0.0471,  0.0000,  0.0062,  ...,  0.0000,  0.0327,  0.0188],\n",
      "        [ 0.0355,  0.0000, -0.0576,  ...,  0.0070,  0.0179,  0.0781],\n",
      "        [-0.0139, -0.0043,  0.0000,  ..., -0.0246,  0.0129, -0.0515]])\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "features = extract_features(model, dataloader)\n",
    "hehe = 'yo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0101,  0.0612,  0.0000,  ...,  0.0527,  0.0000,  0.0000],\n",
       "        [ 0.0371,  0.0000,  0.0000,  ..., -0.0044,  0.0449, -0.0328],\n",
       "        [-0.0331, -0.0598,  0.0000,  ...,  0.0817, -0.0726, -0.0312],\n",
       "        ...,\n",
       "        [ 0.0471,  0.0000,  0.0062,  ...,  0.0000,  0.0327,  0.0188],\n",
       "        [ 0.0355,  0.0000, -0.0576,  ...,  0.0070,  0.0179,  0.0781],\n",
       "        [-0.0139, -0.0043,  0.0000,  ..., -0.0246,  0.0129, -0.0515]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3162258863449097\n",
      "1.5001633167266846\n",
      "1.4585824012756348\n",
      "1.4585890769958496\n",
      "1.4806320667266846\n",
      "1.4065536260604858\n",
      "1.5121769905090332\n",
      "1.4132683277130127\n",
      "1.468139886856079\n",
      "1.3985812664031982\n",
      "1.4092501401901245\n",
      "1.412135362625122\n",
      "1.4351273775100708\n",
      "1.5053290128707886\n",
      "1.4828429222106934\n",
      "1.3329904079437256\n",
      "1.4634788036346436\n",
      "1.4576482772827148\n",
      "1.5322898626327515\n",
      "1.408444881439209\n",
      "1.3793210983276367\n",
      "1.3668891191482544\n",
      "1.3757606744766235\n",
      "1.4014577865600586\n"
     ]
    }
   ],
   "source": [
    "base_feature = features[0]\n",
    "for feature in features[1:]:\n",
    "    distance = euclidean(feature, base_feature)\n",
    "    print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
