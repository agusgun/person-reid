{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Using LFW Dataset - VGGFace (Resnet50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several feature extractor:\n",
    "1. Face Embedding: Facenet\n",
    "2. Face Embedding: VGG Face\n",
    "3. Face Embedding: VGG Face - VGG16\n",
    "4. Face Embedding: VGG Face - RESNET50\n",
    "5. LBPH (Local Binary Pattern Histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agusgun/anaconda3/envs/basic/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras import backend as K\n",
    "from feature_extractor.face_feature_extractor import FaceFeatureExtractor\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = '../lfw/'\n",
    "image_path_list = []\n",
    "labels = []\n",
    "name_dictionary = {}\n",
    "counter = 0\n",
    "for root, dirs, files in os.walk(DIR_PATH):\n",
    "    for filename in files:\n",
    "        person_name = ' '.join(filename.split('.')[0].split('_')[0:-1]) \n",
    "        file_path = os.path.join(root, filename)\n",
    "        if person_name not in name_dictionary:\n",
    "            counter += 1\n",
    "            name_dictionary[person_name] = counter\n",
    "        image_path_list.append(file_path)\n",
    "        labels.append(name_dictionary[person_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13233\n",
      "13233\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(len(image_path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../lfw/Ryan_Newman/Ryan_Newman_0001.jpg',\n",
       " '../lfw/Dimitar_Berbatov/Dimitar_Berbatov_0001.jpg',\n",
       " '../lfw/Ed_Rendell/Ed_Rendell_0001.jpg',\n",
       " '../lfw/Joe_Crede/Joe_Crede_0001.jpg',\n",
       " '../lfw/Norman_Mailer/Norman_Mailer_0001.jpg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = list(zip(image_path_list, labels))\n",
    "random.Random(0).shuffle(temp) # custom seed\n",
    "image_path_list, labels = zip(*temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../lfw/Tommy_Maddox/Tommy_Maddox_0001.jpg',\n",
       " '../lfw/David_Millar/David_Millar_0001.jpg',\n",
       " '../lfw/Gregg_Popovich/Gregg_Popovich_0004.jpg',\n",
       " '../lfw/Shimon_Peres/Shimon_Peres_0001.jpg',\n",
       " '../lfw/Rudolph_Giuliani/Rudolph_Giuliani_0024.jpg')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3038, 1517, 3115, 834, 1016)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dictionary['John Ashcroft']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACENET_MODEL_PATH = 'model_data/facenet/20180402-114759'\n",
    "VGGFACE_MODEL_PATH = 'model_data/vgg_face_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/agusgun/anaconda3/envs/basic/lib/python3.6/site-packages/mtcnn/layer_factory.py:211: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/agusgun/anaconda3/envs/basic/lib/python3.6/site-packages/mtcnn/layer_factory.py:213: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "mtcnn_detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_representation_list, true_label, test_representation, min_distance):\n",
    "    minimum_label = None\n",
    "    minimum_distance = min_distance\n",
    "    \n",
    "    for idx, image_representation in enumerate(image_representation_list):\n",
    "        distance = euclidean(image_representation, test_representation)\n",
    "        if distance < minimum_distance:\n",
    "            minimum_distance = distance\n",
    "            minimum_label = true_label[idx]\n",
    "    \n",
    "    return minimum_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(predictions, labels):\n",
    "    count_same = 0\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        if labels[idx] == prediction:\n",
    "            count_same += 1\n",
    "    return count_same / len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGFace - Resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on experiment VGGFace - Resnet50 have distance below 100 for euclidean if two image are the same person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FaceFeatureExtractor(None, extractor_name='vgg_face_resnet50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_representation_database = []\n",
    "image_representation_labels = []\n",
    "prediction_result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 0\n",
      "Checkpoint 1000\n",
      "Checkpoint 2000\n",
      "Checkpoint 3000\n",
      "Checkpoint 4000\n",
      "Checkpoint 5000\n",
      "Checkpoint 6000\n",
      "Checkpoint 7000\n",
      "Checkpoint 8000\n",
      "Checkpoint 9000\n",
      "Checkpoint 10000\n",
      "Checkpoint 11000\n",
      "Checkpoint 12000\n",
      "Checkpoint 13000\n",
      "CPU times: user 3h 55min 7s, sys: 1h 55min 24s, total: 5h 50min 32s\n",
      "Wall time: 1h 12min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for idx, image_path in enumerate(image_path_list):\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"Checkpoint\", idx)\n",
    "    img = cv.cvtColor(cv.imread(image_path), cv.COLOR_BGR2RGB)\n",
    "    detection_result = mtcnn_detector.detect_faces(img)\n",
    "    cropped_image = None\n",
    "    for face in detection_result:\n",
    "        face_bbox = face['box']\n",
    "        x, y, w, h = face_bbox\n",
    "        if x < 0:\n",
    "            x = 0\n",
    "        if y < 0:\n",
    "            y = 0\n",
    "        cropped_image = img[y:y+h, x:x+w]\n",
    "        break\n",
    "    \n",
    "    if not cropped_image is None:\n",
    "        feature_test = feature_extractor.extract_image(cropped_image)\n",
    "        prediction = predict(image_representation_database, image_representation_labels, feature_test, THRESHOLD)\n",
    "        if prediction == None:\n",
    "            label = labels[idx]\n",
    "            if label not in image_representation_labels:\n",
    "                image_representation_labels.append(label)\n",
    "                image_representation_database.append(feature_test)\n",
    "                prediction_result.append(label)\n",
    "            else: # false prediction\n",
    "                prediction_result.append(-1) \n",
    "        else:\n",
    "            prediction_result.append(prediction)\n",
    "    else: # failed to detect faces\n",
    "        prediction_result.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , ..., 0.06914644, 0.        ,\n",
       "       0.17514291], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor.extract_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../lfw/George_W_Bush/George_W_Bush_0233.jpg'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path_list[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7877276505705434"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_score(prediction_result, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13233"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13233"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_extractor.extractor.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_list = sorted(image_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../lfw/Abdullah_Gul/Abdullah_Gul_0001.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0002.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0003.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0004.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0005.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0006.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0007.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0008.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0009.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0010.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0011.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0012.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0013.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0014.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0015.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0016.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0017.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0018.jpg',\n",
       " '../lfw/Abdullah_Gul/Abdullah_Gul_0019.jpg']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path_list[31:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 0\n",
      "CPU times: user 1.19 s, sys: 623 ms, total: 1.81 s\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for idx, image_path in enumerate(image_path_list[31:32]):\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"Checkpoint\", idx)\n",
    "    img = cv.cvtColor(cv.imread(image_path), cv.COLOR_BGR2RGB)\n",
    "    detection_result = mtcnn_detector.detect_faces(img)\n",
    "    cropped_image = None\n",
    "    for face in detection_result:\n",
    "        face_bbox = face['box']\n",
    "        x, y, w, h = face_bbox\n",
    "        if x < 0:\n",
    "            x = 0\n",
    "        if y < 0:\n",
    "            y = 0\n",
    "        cropped_image = img[y:y+h, x:x+w]\n",
    "        break\n",
    "    \n",
    "    if not cropped_image is None:\n",
    "        feature_test = feature_extractor.extract_image(cropped_image)\n",
    "        prediction = predict(image_representation_database, image_representation_labels, feature_test, THRESHOLD)\n",
    "        if prediction == None:\n",
    "            label = labels[idx]\n",
    "            if label not in image_representation_labels:\n",
    "                image_representation_labels.append(label)\n",
    "                image_representation_database.append(feature_test)\n",
    "                prediction_result.append(label)\n",
    "            else: # false prediction\n",
    "                prediction_result.append(-1) \n",
    "        else:\n",
    "            prediction_result.append(prediction)\n",
    "    else: # failed to detect faces\n",
    "        prediction_result.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.64275444, 0.16716784, ..., 0.01909242, 2.3434634 ,\n",
       "       0.49065006], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_feature = feature_test.copy()\n",
    "base_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 0\n",
      "0.0\n",
      "82.52213287353516\n",
      "84.78237915039062\n",
      "78.82715606689453\n",
      "79.36217498779297\n",
      "60.43344497680664\n",
      "48.81595993041992\n",
      "69.8621826171875\n",
      "106.51448822021484\n",
      "67.05977630615234\n",
      "58.84721755981445\n",
      "52.67277526855469\n",
      "52.3817253112793\n",
      "73.55290222167969\n",
      "65.49700164794922\n",
      "78.40837097167969\n",
      "60.621822357177734\n",
      "72.69193267822266\n",
      "67.33021545410156\n",
      "CPU times: user 22 s, sys: 12.2 s, total: 34.1 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for idx, image_path in enumerate(image_path_list[31:50]):\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"Checkpoint\", idx)\n",
    "    img = cv.cvtColor(cv.imread(image_path), cv.COLOR_BGR2RGB)\n",
    "    detection_result = mtcnn_detector.detect_faces(img)\n",
    "    cropped_image = None\n",
    "    for face in detection_result:\n",
    "        face_bbox = face['box']\n",
    "        x, y, w, h = face_bbox\n",
    "        if x < 0:\n",
    "            x = 0\n",
    "        if y < 0:\n",
    "            y = 0\n",
    "        cropped_image = img[y:y+h, x:x+w]\n",
    "        break\n",
    "    \n",
    "    if not cropped_image is None:\n",
    "        feature_test = feature_extractor.extract_image(cropped_image)\n",
    "        distance = euclidean(feature_test, base_feature)\n",
    "        print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 0\n",
      "136.31793212890625\n",
      "138.59767150878906\n",
      "136.0935516357422\n",
      "139.9056396484375\n",
      "126.19060516357422\n",
      "129.37594604492188\n",
      "135.495361328125\n",
      "131.88719177246094\n",
      "142.462646484375\n",
      "128.8269805908203\n",
      "133.00912475585938\n",
      "121.98558807373047\n",
      "130.2308807373047\n",
      "130.763916015625\n",
      "130.13465881347656\n",
      "126.12809753417969\n",
      "141.56590270996094\n",
      "133.5976104736328\n",
      "142.20700073242188\n",
      "121.29850006103516\n",
      "127.96356201171875\n",
      "123.25005340576172\n",
      "134.73443603515625\n",
      "125.16188049316406\n",
      "132.94863891601562\n",
      "122.6933822631836\n",
      "114.6954574584961\n",
      "137.6082763671875\n",
      "115.60914611816406\n",
      "122.6716537475586\n",
      "CPU times: user 32.7 s, sys: 16.8 s, total: 49.5 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for idx, image_path in enumerate(image_path_list[0:30]):\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"Checkpoint\", idx)\n",
    "    img = cv.cvtColor(cv.imread(image_path), cv.COLOR_BGR2RGB)\n",
    "    detection_result = mtcnn_detector.detect_faces(img)\n",
    "    cropped_image = None\n",
    "    for face in detection_result:\n",
    "        face_bbox = face['box']\n",
    "        x, y, w, h = face_bbox\n",
    "        if x < 0:\n",
    "            x = 0\n",
    "        if y < 0:\n",
    "            y = 0\n",
    "        cropped_image = img[y:y+h, x:x+w]\n",
    "        break\n",
    "    \n",
    "    if not cropped_image is None:\n",
    "        feature_test = feature_extractor.extract_image(cropped_image)\n",
    "        distance = euclidean(feature_test, base_feature)\n",
    "        print(distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
